#!/bin/bash

DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" && pwd )"
if [ -z $1 ] || [ ! -r $DIR/$1 ]
  then
    configfile="$DIR/backup.cfg"
  else
    configfile="$DIR/$1"
fi

source "$configfile"
dow=`/bin/date +%w`
dom=`/bin/date +%d`
month=`/bin/date +%m`

# Add the param "firstrun" to force a full backup instead of incremental.
if [ "$2" = 'firstrun' ] || ([ -z $2 ] && [ "$1" = 'firstrun' ])
  then
    dom='01'
    month='01'
    firstrun=TRUE
fi

# Pull in variables from the config file.
export PASSPHRASE=$passphrase
if [ ! -z $s3accesskey ]
  then
    export AWS_ACCESS_KEY_ID=$s3accesskey
    export AWS_SECRET_ACCESS_KEY=$s3secret
fi

# FILESYSTEM BACKUP
if [ $dom = '01' ]
  # Create a full duplicity backup if it's the first of the month.
  then
    # Create a local duplicity backup of the files directory 
    lastrun=`duplicity full --name $siteid $fileroot file://$local/files`
    # Delete old duplicity backups.
    # If this is run on September 1st, it will keep backups from April 1, May 1, June 1, July 1, and every day in August.
    lastrun+=$'\n'`duplicity remove-all-but-n-full 6 --name $siteid --force file://$local/files`
    lastrun+=$'\n'`duplicity remove-all-inc-of-but-n-full 2 --name $siteid --force file://$local/files`
    # If you've defined an off-site remote host, make duplicity backups there, too.
    if [ ! -z $remote ]
      then
        lastrun+=$'\n'`duplicity full --name $siteid-remote-files $fileroot $remote/files`
        lastrun+=$'\n'`duplicity remove-all-but-n-full 6 --name $siteid-remote-files --force $remote/files`
        lastrun+=$'\n'`duplicity remove-all-inc-of-but-n-full 2 --name $siteid-remote-files --force $remote/files`
    fi
  # It's not the first of the month - just do an incremental backup (plus a remote incremental backup if remote is defined).  
  else
    lastrun=`duplicity incremental --name $siteid $fileroot file://$local/files`
    if [ ! -z $remote ]
      then
        lastrun+=$'\n'`duplicity incremental --name $siteid-remote-files $fileroot $remote/files`
    fi
fi

# SQL BACKUP

# Make folders to store db backups if need be.
mkdir -p $local/db/daily
mkdir -p $local/db/weekly
mkdir -p $local/db/monthly

# Delete the daily backup and create a new one.
rm -f $local/db/$dbname"daily"*.sql
if [ -z $loginpath ]
  then
    mysqldump -u $dbuser -p$dbpass $dbname > $local/db/$dbname"daily"`/bin/date +%Y%m%d`.sql
  else
    mysqldump --login-path=$loginpath $dbname > $local/db/$dbname"daily"`/bin/date +%Y%m%d`.sql
fi

# Make a .tar.gz of the directory and tighten its permissions.
tar -C $local/db -czf $local/db/daily/$dbname"daily"`/bin/date +%Y%m%d`.sql.tar.gz $dbname"daily"`/bin/date +%Y%m%d`.sql
chmod 600 $local/db/$dbname"daily"`/bin/date +%Y%m%d`.sql
chmod 600 $local/db/daily/$dbname"daily"`/bin/date +%Y%m%d`.sql.tar.gz

# If you have a second database defined for your CMS (i.e. split CiviCRM/CMS dbs, back up the CMS db also.
# We back up the parent folder, not the file itself, so we can give each backup a unique name (a timestamp)
# while still allowing duplicity to remove older backups.
if [ ! -z $cmsdbname ]
  then
    rm -f $local/db/$cmsdbname"daily"*.sql
    if [ -z $loginpath ]
      then
        mysqldump -u $dbuser -p$dbpass $cmsdbname > $local/db/$cmsdbname"daily"`/bin/date +%Y%m%d`.sql
      else
        mysqldump --login-path=$loginpath $cmsdbname > $local/db/$cmsdbname"daily"`/bin/date +%Y%m%d`.sql
    fi

    tar -C $local/db -czf $local/db/daily/$cmsdbname"daily"`/bin/date +%Y%m%d`.sql.tar.gz $cmsdbname"daily"`/bin/date +%Y%m%d`.sql
    chmod 600 $local/db/$cmsdbname"daily"`/bin/date +%Y%m%d`.sql
    chmod 600 $local/db/daily/$cmsdbname"daily"`/bin/date +%Y%m%d`.sql.tar.gz
fi

# If you have a remote defined, Make an off-site copy of the daily db backup.
if [ ! -z $remote ]
  then
    if [ $dom = '01' ]
      then
        lastrun+=$'\n'`duplicity full --name $siteid-db-daily $local/db/daily $remote/db/daily`
      else
        lastrun+=$'\n'`duplicity incremental --name $siteid-db-daily $local/db/daily $remote/db/daily`
    fi
fi

# On Sundays, move a copy of the daily backup archive to the weekly folder.
if [ $dow -eq 0 ]
  then
    mv $local/db/daily/$dbname"daily"`/bin/date +%Y%m%d`.sql.tar.gz $local/db/weekly/$dbname"weekly"`/bin/date +%Y%m%d`.sql.tar.gz
    rm -f $local/db/daily/$dbname"daily"*

    if [ ! -z $cmsdbname ]
      then
        mv $local/db/daily/$cmsdbname"daily"`/bin/date +%Y%m%d`.sql.tar.gz $local/db/weekly/$cmsdbname"weekly"`/bin/date +%Y%m%d`.sql.tar.gz
        rm -f $local/db/daily/$cmsdbname"daily"*
    fi
fi

# On the first of the month, move the backup archive from last Sunday to the monthly folder.
# Why not the most recent backup?  I don't 100% understand why daily/weekly files are deleted when they are.
if [ $dom = '01' ]
  then
    if [ -z $firstrun ]
      then
        mv $local/db/weekly/$dbname"weekly"`/bin/date --date='last sunday' +%Y%m%d`.sql.tar.gz $local/db/monthly/$dbname"monthly"`/bin/date --date='last sunday' +%Y%m%d`.sql.tar.gz
        rm -f $local/db/weekly/$dbname"weekly"*

        if [ ! -z $cmsdbname ]
          then
            mv $local/db/weekly/$cmsdbname"weekly"`/bin/date --date='last sunday' +%Y%m%d`.sql.tar.gz $local/db/monthly/$cmsdbname"monthly"`/bin/date --date='last sunday' +%Y%m%d`.sql.tar.gz
            rm -f $local/db/weekly/$cmsdbname"weekly"*
        fi
    fi
    # Copy db backups to the off-site backup host.  But only once a month.
    if [ ! -z $remote ]
      then
        lastrun+=$'\n'`duplicity remove-all-but-n-full 2 --name $siteid-db-daily --force $remote/db/daily`
        lastrun+=$'\n'`duplicity remove-all-but-n-full 1 --name $siteid-db-monthly --force $remote/db/monthly`

        if [ $month = '01' ]
          then
            lastrun+=$'\n'`duplicity full --name $siteid-db-monthly $local/db/monthly $remote/db/monthly`
          else
            lastrun+=$'\n'`duplicity incremental --name $siteid-db-monthly $local/db/monthly $remote/db/monthly`
        fi
    fi
fi
unset PASSPHRASE
if [ ! -z $s3accesskey ]
  then
    unset AWS_ACCESS_KEY_ID
    unset AWS_SECRET_ACCESS_KEY
fi

echo "$lastrun" > $DIR/lastrun
